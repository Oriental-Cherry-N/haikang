# 小样本工业缺陷检测技术文档

## 基于 Anomalib 0.7 的自适应异常检测系统

---

## 目录

1. [技术挑战与解决方案概述](#1-技术挑战与解决方案概述)
2. [核心算法架构](#2-核心算法架构)
3. [小样本学习策略](#3-小样本学习策略)
4. [领域自适应与分布偏移处理](#4-领域自适应与分布偏移处理)
5. [可解释性与自适应推理机制](#5-可解释性与自适应推理机制)
6. [系统实现细节](#6-系统实现细节)
7. [算法选型对比与推荐](#7-算法选型对比与推荐)
8. [Demo 使用指南](#8-demo-使用指南)

---

## 1. 技术挑战与解决方案概述

### 1.1 问题定义

在工业缺陷检测场景中，我们面临以下核心挑战：

| 挑战 | 描述 | 影响 |
|------|------|------|
| **小样本问题** | 缺陷样本稀缺（通常 <100 张） | 传统监督学习方法失效 |
| **产品变型** | 不同批次产品存在外观差异 | 模型泛化能力下降 |
| **分布偏移** | 训练与部署环境差异 | 误报率和漏报率升高 |
| **可解释性** | 黑盒模型难以被工业界接受 | 难以进行质量追溯 |

### 1.2 解决方案框架

```
┌─────────────────────────────────────────────────────────────────┐
│                    小样本缺陷检测系统架构                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐      │
│  │  数据增强层   │ -> │  特征提取层   │ -> │  异常检测层   │      │
│  │              │    │              │    │              │      │
│  │ • 几何变换    │    │ • 预训练主干  │    │ • Memory Bank│      │
│  │ • 颜色增强    │    │ • 多尺度特征  │    │ • KNN Score  │      │
│  │ • 合成缺陷    │    │ • 特征金字塔  │    │ • 阈值自适应 │      │
│  └──────────────┘    └──────────────┘    └──────────────┘      │
│                                                                 │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐      │
│  │  域自适应层   │ -> │  后处理层     │ -> │  可解释层    │      │
│  │              │    │              │    │              │      │
│  │ • 特征对齐    │    │ • 分数归一化  │    │ • 热力图生成 │      │
│  │ • 风格迁移    │    │ • 阈值优化    │    │ • 注意力可视化│      │
│  │ • 增量学习    │    │ • 形态学处理  │    │ • 置信度评估 │      │
│  └──────────────┘    └──────────────┘    └──────────────┘      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. 核心算法架构

### 2.1 基于记忆库的异常检测 (PatchCore)

**核心思想**：仅使用正常样本构建特征记忆库，通过计算测试样本与记忆库的距离来判断异常。

#### 2.1.1 算法流程

```
输入图像 -> 预训练特征提取 -> 特征降维 -> 核心集采样 -> 记忆库存储
                                                           ↓
推理阶段: 输入图像 -> 特征提取 -> K近邻匹配 -> 异常分数计算 -> 热力图生成
```

#### 2.1.2 数学原理

**特征提取**：
$$F = \phi(x) \in \mathbb{R}^{H \times W \times C}$$

其中 $\phi$ 为预训练网络（如 ResNet），$x$ 为输入图像。

**异常分数计算**：
$$s(x) = \max_{p \in P} \min_{m \in M} \|f_p - m\|_2$$

其中 $P$ 为测试图像的 patch 集合，$M$ 为记忆库，$f_p$ 为 patch $p$ 的特征向量。

**核心集采样**（Coreset Subsampling）：
为减少记忆库大小，采用贪心核心集算法：
$$m^* = \arg\max_{m \in M \setminus S} \min_{s \in S} \|m - s\|_2$$

### 2.2 为什么适合小样本场景？

| 优势 | 说明 |
|------|------|
| **仅需正常样本** | 无需缺陷样本参与训练 |
| **一次性学习** | 只需 1 个 epoch，快速建立记忆库 |
| **预训练知识迁移** | 利用 ImageNet 预训练权重 |
| **非参数化方法** | 不存在过拟合风险 |

---

## 3. 小样本学习策略

### 3.1 数据增强策略

针对小样本场景，我们设计了多层次数据增强方案：

#### 3.1.1 几何变换增强

**目的**：模拟不同拍摄角度、位置和形变，增加数据多样性。

**适用场景**：产品可能以不同方向放置，或相机角度有轻微变化。

```python
# ============================================================================
# 几何变换增强配置
# ============================================================================
# 安装: pip install albumentations
# 文档: https://albumentations.ai/docs/
# ============================================================================

import albumentations as A
import cv2
import numpy as np

# 定义几何变换组合
# A.Compose 会按顺序应用各个变换，每个变换根据概率 p 决定是否执行
geometric_transform = A.Compose([
    
    # ------ 翻转变换 ------
    # 水平翻转：将图像左右镜像
    # p=0.5 表示有 50% 的概率执行此变换
    # 适用于：产品无左右方向性（如圆形零件、对称产品）
    # 不适用于：有方向性的产品（如文字、箭头标识）
    A.HorizontalFlip(p=0.5),
    
    # 垂直翻转：将图像上下镜像
    # 适用于：产品无上下方向性
    # 注意：某些产品可能有固定朝向，慎用
    A.VerticalFlip(p=0.5),
    
    # ------ 旋转变换 ------
    # 随机 90 度旋转：0°、90°、180°、270° 四种角度随机选择
    # 适用于：产品可任意旋转放置的场景
    # 优点：不产生插值伪影（像素精确对齐）
    A.RandomRotate90(p=0.5),
    
    # ------ 仿射变换组合 ------
    # 同时进行平移、缩放、旋转的组合变换
    A.ShiftScaleRotate(
        shift_limit=0.0625,   # 平移范围：图像尺寸的 ±6.25%
                              # 例如 512x512 图像，最大平移 32 像素
        scale_limit=0.1,      # 缩放范围：±10%（即 0.9x ~ 1.1x）
                              # 模拟相机距离变化
        rotate_limit=15,      # 旋转范围：±15 度
                              # 模拟轻微的相机倾斜
        border_mode=cv2.BORDER_CONSTANT,  # 边界填充模式：常数填充（黑色）
                              # 其他选项：BORDER_REFLECT（反射）, BORDER_REPLICATE（复制边缘）
        value=0,              # 填充值（当 border_mode=CONSTANT 时）
        p=0.5                 # 50% 概率应用
    ),
    
    # ------ 弹性变换 ------
    # 模拟物体的轻微形变（如柔性材料、曲面反射）
    # 原理：生成随机位移场，对图像进行非线性扭曲
    A.ElasticTransform(
        alpha=120,            # 位移场的强度系数
                              # 值越大，形变越明显
        sigma=120 * 0.05,     # 高斯模糊的标准差（平滑位移场）
                              # sigma = alpha * 0.05 是经验值
        alpha_affine=120 * 0.03,  # 仿射变换的强度
                              # 控制整体扭曲程度
        border_mode=cv2.BORDER_CONSTANT,
        value=0,
        p=0.3                 # 30% 概率应用（弹性变换较激进，概率设低一些）
    ),
])

# ============================================================================
# 【如何嵌入训练代码】
# ============================================================================
# 
# 方法1：直接传入 Anomalib 的 Folder 数据模块
# ----------------------------------------------------------------------------
# from anomalib.data.folder import Folder
# 
# datamodule = Folder(
#     normal_dir="./datasets/task_1/good",
#     abnormal_dir="./datasets/task_1/defect",
#     mask_dir="./datasets/task_1/mask/defect",
#     image_size=(512, 512),
#     transform_config_train=geometric_transform,  # <-- 传入这里
#     task=TaskType.SEGMENTATION,
# )
# 
# ----------------------------------------------------------------------------
# 
# 方法2：手动应用变换（用于自定义数据加载）
# ----------------------------------------------------------------------------
# def load_and_augment(image_path):
#     # 读取图像
#     image = cv2.imread(image_path)
#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#     
#     # 应用变换
#     transformed = geometric_transform(image=image)
#     augmented_image = transformed['image']
#     
#     return augmented_image
# 
# ============================================================================
```

#### 3.1.2 颜色/光照增强

**目的**：模拟不同光照条件、相机噪声和轻微失焦，提高模型对光照变化的鲁棒性。

**适用场景**：生产线光照条件可能变化、不同时段光线不同、相机曝光参数波动。

```python
# ============================================================================
# 颜色/光照增强配置
# ============================================================================
# 工业场景中，光照变化是导致误检的主要因素之一
# 通过模拟各种光照条件，使模型对光照变化不敏感
# ============================================================================

import albumentations as A

color_transform = A.Compose([
    
    # ------ 颜色抖动 ------
    # 同时随机调整亮度、对比度、饱和度、色调
    # 模拟不同光源色温、相机白平衡变化
    A.ColorJitter(
        brightness=0.2,   # 亮度变化范围：±20%
                          # 0.2 表示亮度在 [0.8, 1.2] 倍之间变化
        contrast=0.2,     # 对比度变化范围：±20%
                          # 模拟不同光照强度下的对比度差异
        saturation=0.2,   # 饱和度变化范围：±20%
                          # 模拟相机色彩设置差异
        hue=0.1,          # 色调变化范围：±10%（相对于色相环）
                          # 注意：工业场景色调变化通常较小，不宜设太大
        p=0.5             # 50% 概率应用
    ),
    
    # ------ 亮度对比度调整 ------
    # 比 ColorJitter 更简单，只调整亮度和对比度
    # 可与 ColorJitter 叠加使用，增加变化多样性
    A.RandomBrightnessContrast(
        brightness_limit=0.2,  # 亮度变化：±20%
        contrast_limit=0.2,    # 对比度变化：±20%
        brightness_by_max=True,  # True: 相对于最大像素值调整
        p=0.5
    ),
    
    # ------ 高斯噪声 ------
    # 模拟相机传感器噪声（ISO 高时噪声大）
    # 适用于：低光照环境、高 ISO 拍摄
    A.GaussNoise(
        var_limit=(10.0, 50.0),  # 噪声方差范围
                                  # 值越大噪声越明显
                                  # 10-50 是适中范围，不会完全破坏图像
        mean=0,                   # 噪声均值（通常为0）
        per_channel=True,         # True: 每个通道独立添加噪声
                                  # 更真实地模拟彩色噪声
        p=0.3                     # 30% 概率（噪声不宜过多）
    ),
    
    # ------ 高斯模糊 ------
    # 模拟轻微失焦或运动模糊
    # 适用于：相机对焦不稳定、产品轻微晃动
    A.GaussianBlur(
        blur_limit=(3, 7),  # 模糊核大小范围：3x3 到 7x7
                            # 必须为奇数
                            # 3: 轻微模糊，7: 较明显模糊
        sigma_limit=0,      # 0 表示自动计算 sigma
        p=0.3               # 30% 概率（过度模糊会损失细节）
    ),
])

# ============================================================================
# 【组合几何变换和颜色变换】
# ============================================================================
# 在实际使用中，通常将几何变换和颜色变换组合在一起
# 注意：先做几何变换，再做颜色变换（顺序影响结果）
# ============================================================================

# 完整的训练数据增强流水线
train_transform = A.Compose([
    # === 第一阶段：几何变换 ===
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(
        shift_limit=0.05,
        scale_limit=0.1,
        rotate_limit=15,
        p=0.5
    ),
    
    # === 第二阶段：颜色/光照变换 ===
    A.RandomBrightnessContrast(
        brightness_limit=0.2,
        contrast_limit=0.2,
        p=0.5
    ),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
    A.GaussianBlur(blur_limit=(3, 5), p=0.2),
])

# ============================================================================
# 【如何嵌入训练代码 - 完整示例】
# ============================================================================

"""
# 在 train.py 中使用：

from anomalib.data.folder import Folder
from anomalib.data.task_type import TaskType
import albumentations as A

# 定义数据增强
train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
    A.GaussianBlur(blur_limit=(3, 5), p=0.2),
])

# 创建数据模块时传入
datamodule = Folder(
    normal_dir="./datasets/task_1/good",
    abnormal_dir="./datasets/task_1/defect", 
    mask_dir="./datasets/task_1/mask/defect",
    image_size=(512, 512),
    train_batch_size=1,
    num_workers=8,
    task=TaskType.SEGMENTATION,
    transform_config_train=train_transform,  # 训练时应用增强
    # transform_config_eval=None,            # 测试时不增强（默认）
)
"""
```

#### 3.1.3 合成缺陷增强 (DRAEM 思想)

**目的**：当正常样本极度稀缺时，通过合成异常来创造更多训练数据。

**原理**：使用 Perlin 噪声生成随机形状的掩码，将异常纹理融合到正常图像上。

**适用场景**：正常样本 < 30 张，需要强化模型对异常的识别能力。

```python
# ============================================================================
# 合成缺陷增强 (基于 DRAEM 思想)
# ============================================================================
# 论文: "DRAEM - A discriminatively trained reconstruction embedding 
#        for surface anomaly detection" (ICCV 2021)
# 
# 核心思想：
# 1. 使用 Perlin 噪声生成随机形状的异常区域掩码
# 2. 从外部纹理库选择异常纹理
# 3. 将异常纹理融合到正常图像上，生成合成缺陷样本
# ============================================================================

import numpy as np
import cv2
import random
from pathlib import Path
from typing import Tuple, List, Optional


class SyntheticAnomalyGenerator:
    """
    合成异常生成器
    
    在正常图像上合成各种类型的人工缺陷，用于数据增强。
    
    使用示例:
    ---------
    >>> generator = SyntheticAnomalyGenerator(
    ...     anomaly_source_path="./textures/dtd"  # 可选：异常纹理库路径
    ... )
    >>> anomaly_image, mask = generator.generate(normal_image)
    
    参数说明:
    ---------
    anomaly_source_path : str, optional
        异常纹理图像的目录路径。
        推荐使用 DTD (Describable Textures Dataset) 数据集。
        下载地址: https://www.robots.ox.ac.uk/~vgg/data/dtd/
        如果不提供，将使用程序生成的随机纹理。
    """
    
    def __init__(self, anomaly_source_path: Optional[str] = None):
        """
        初始化合成异常生成器
        
        Args:
            anomaly_source_path: 异常纹理库路径（可选）
                                 如果提供，将从该目录加载纹理图像
                                 如果不提供，使用程序生成的随机纹理
        """
        # 初始化 Perlin 噪声生成器（用于生成自然形状的掩码）
        self.perlin_scale = 6  # 噪声缩放因子，影响掩码形状的复杂度
        
        # 加载异常纹理库
        self.anomaly_textures = self._load_textures(anomaly_source_path)
        
        # 异常类型权重（控制不同类型异常的生成概率）
        self.anomaly_types = {
            'texture': 0.4,    # 纹理替换（最常见）
            'color': 0.3,      # 颜色异常
            'noise': 0.2,      # 噪声异常
            'patch': 0.1,      # 块状异常
        }
    
    def _load_textures(self, path: Optional[str]) -> List[np.ndarray]:
        """
        加载异常纹理库
        
        Args:
            path: 纹理图像目录路径
        
        Returns:
            纹理图像列表
        """
        textures = []
        
        if path is not None and Path(path).exists():
            # 从目录加载所有图像
            texture_dir = Path(path)
            extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
            
            for ext in extensions:
                for img_path in texture_dir.rglob(ext):
                    img = cv2.imread(str(img_path))
                    if img is not None:
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        textures.append(img)
            
            print(f"[SyntheticAnomalyGenerator] 加载了 {len(textures)} 个纹理图像")
        
        # 如果没有加载到纹理，生成一些默认纹理
        if len(textures) == 0:
            print("[SyntheticAnomalyGenerator] 未找到外部纹理，使用程序生成的纹理")
            textures = self._generate_default_textures()
        
        return textures
    
    def _generate_default_textures(self) -> List[np.ndarray]:
        """
        生成默认的异常纹理（当没有外部纹理库时使用）
        
        Returns:
            程序生成的纹理图像列表
        """
        textures = []
        size = 256
        
        # 生成不同类型的纹理
        for _ in range(20):
            # 随机噪声纹理
            noise = np.random.randint(0, 255, (size, size, 3), dtype=np.uint8)
            textures.append(noise)
            
            # 条纹纹理
            stripe = np.zeros((size, size, 3), dtype=np.uint8)
            stripe_width = random.randint(5, 20)
            for i in range(0, size, stripe_width * 2):
                stripe[i:i+stripe_width] = [random.randint(0, 255) for _ in range(3)]
            textures.append(stripe)
            
            # 渐变纹理
            gradient = np.zeros((size, size, 3), dtype=np.uint8)
            for i in range(size):
                gradient[i] = [int(255 * i / size)] * 3
            textures.append(gradient)
        
        return textures
    
    def _generate_perlin_mask(self, shape: Tuple[int, int]) -> np.ndarray:
        """
        使用 Perlin 噪声生成自然形状的异常掩码
        
        Perlin 噪声的特点：
        - 产生连续、平滑的随机值
        - 生成的形状更自然，类似真实缺陷
        - 可控制形状的复杂度（通过缩放因子）
        
        Args:
            shape: 目标掩码尺寸 (H, W)
        
        Returns:
            二值掩码，形状为 [H, W]，值为 0 或 1
        """
        h, w = shape
        
        # 简化版 Perlin 噪声实现
        # 生成低分辨率噪声，然后上采样
        scale = self.perlin_scale
        noise_h, noise_w = h // (2 ** scale), w // (2 ** scale)
        noise_h = max(noise_h, 2)
        noise_w = max(noise_w, 2)
        
        # 生成多尺度噪声并叠加（模拟 Perlin 噪声的分形特性）
        noise = np.zeros((h, w), dtype=np.float32)
        for octave in range(4):
            freq = 2 ** octave
            amp = 1.0 / freq
            
            # 生成当前尺度的噪声
            octave_noise = np.random.randn(noise_h * freq, noise_w * freq)
            
            # 上采样到目标尺寸
            octave_noise = cv2.resize(octave_noise, (w, h), interpolation=cv2.INTER_CUBIC)
            
            noise += octave_noise * amp
        
        # 归一化到 [0, 1]
        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8)
        
        # 随机阈值二值化，生成掩码
        threshold = random.uniform(0.4, 0.6)
        mask = (noise > threshold).astype(np.float32)
        
        # 可选：形态学操作使掩码更平滑
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # 确保掩码不为空且不覆盖整个图像
        mask_ratio = mask.sum() / mask.size
        if mask_ratio < 0.01 or mask_ratio > 0.5:
            # 如果掩码太小或太大，使用简单的圆形/椭圆形掩码
            mask = self._generate_simple_mask(shape)
        
        return mask
    
    def _generate_simple_mask(self, shape: Tuple[int, int]) -> np.ndarray:
        """
        生成简单形状的掩码（椭圆或矩形）
        作为 Perlin 掩码的后备方案
        
        Args:
            shape: 目标尺寸 (H, W)
        
        Returns:
            二值掩码
        """
        h, w = shape
        mask = np.zeros((h, w), dtype=np.float32)
        
        # 随机选择形状类型
        shape_type = random.choice(['ellipse', 'rectangle', 'polygon'])
        
        # 随机位置和大小
        center_x = random.randint(w // 4, 3 * w // 4)
        center_y = random.randint(h // 4, 3 * h // 4)
        size_x = random.randint(w // 10, w // 4)
        size_y = random.randint(h // 10, h // 4)
        
        if shape_type == 'ellipse':
            cv2.ellipse(mask, (center_x, center_y), (size_x, size_y), 
                       random.randint(0, 180), 0, 360, 1.0, -1)
        elif shape_type == 'rectangle':
            pt1 = (center_x - size_x, center_y - size_y)
            pt2 = (center_x + size_x, center_y + size_y)
            cv2.rectangle(mask, pt1, pt2, 1.0, -1)
        else:  # polygon
            n_points = random.randint(5, 8)
            angles = np.linspace(0, 2*np.pi, n_points, endpoint=False)
            radii = [random.uniform(0.5, 1.0) * min(size_x, size_y) for _ in range(n_points)]
            points = [(int(center_x + r * np.cos(a)), int(center_y + r * np.sin(a))) 
                     for r, a in zip(radii, angles)]
            cv2.fillPoly(mask, [np.array(points)], 1.0)
        
        return mask
    
    def generate(self, normal_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        在正常图像上合成异常
        
        Args:
            normal_image: 正常图像，形状为 [H, W, 3]，RGB 格式
        
        Returns:
            anomaly_image: 合成的异常图像
            mask: 异常区域掩码（255=异常，0=正常）
        """
        h, w = normal_image.shape[:2]
        
        # Step 1: 生成 Perlin 噪声掩码
        mask = self._generate_perlin_mask((h, w))
        
        # Step 2: 选择并准备异常纹理
        texture = random.choice(self.anomaly_textures)
        texture = cv2.resize(texture, (w, h))
        
        # Step 3: 可选的纹理增强（增加多样性）
        if random.random() > 0.5:
            # 随机旋转纹理
            angle = random.randint(0, 360)
            M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)
            texture = cv2.warpAffine(texture, M, (w, h))
        
        if random.random() > 0.5:
            # 随机调整纹理亮度
            factor = random.uniform(0.5, 1.5)
            texture = np.clip(texture * factor, 0, 255).astype(np.uint8)
        
        # Step 4: 融合生成异常图像
        # 将掩码扩展到3通道
        mask_3ch = np.stack([mask] * 3, axis=-1)
        
        # 使用 alpha 混合融合
        beta = random.uniform(0.6, 1.0)  # 异常融合强度
        anomaly = normal_image * (1 - mask_3ch * beta) + texture * (mask_3ch * beta)
        anomaly = np.clip(anomaly, 0, 255).astype(np.uint8)
        
        # Step 5: 将掩码转换为标准格式（0-255）
        mask_output = (mask * 255).astype(np.uint8)
        
        return anomaly, mask_output


# ============================================================================
# 【如何嵌入训练代码】
# ============================================================================

"""
# 方法1：在自定义数据集中使用
# ----------------------------------------------------------------------------

from torch.utils.data import Dataset

class AugmentedDataset(Dataset):
    def __init__(self, normal_images, use_synthetic=True):
        self.normal_images = normal_images
        self.use_synthetic = use_synthetic
        
        if use_synthetic:
            # 初始化合成异常生成器
            self.anomaly_generator = SyntheticAnomalyGenerator(
                anomaly_source_path="./textures/dtd/images"  # DTD 数据集路径
            )
    
    def __getitem__(self, idx):
        image = self.normal_images[idx]
        
        # 以一定概率生成合成异常
        if self.use_synthetic and random.random() > 0.5:
            image, mask = self.anomaly_generator.generate(image)
            label = 1  # 异常
        else:
            mask = np.zeros(image.shape[:2], dtype=np.uint8)
            label = 0  # 正常
        
        return image, mask, label

# ----------------------------------------------------------------------------

# 方法2：作为数据增强的一部分（配合 Albumentations）
# ----------------------------------------------------------------------------

import albumentations as A
from albumentations.core.transforms_interface import ImageOnlyTransform

class SyntheticAnomalyTransform(ImageOnlyTransform):
    '''将合成异常生成器包装为 Albumentations 变换'''
    
    def __init__(self, anomaly_source_path=None, always_apply=False, p=0.5):
        super().__init__(always_apply, p)
        self.generator = SyntheticAnomalyGenerator(anomaly_source_path)
    
    def apply(self, img, **params):
        anomaly, _ = self.generator.generate(img)
        return anomaly

# 使用示例
train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    SyntheticAnomalyTransform(p=0.3),  # 30% 概率生成合成异常
    A.RandomBrightnessContrast(p=0.5),
])
"""
```

### 3.2 特征级增强

#### 3.2.1 多尺度特征融合

**目的**：从 CNN 的不同层提取特征，融合不同粗细粒度的信息。

**原理**：
- 浅层特征（layer1/2）：包含纹理、边缘等局部细节信息
- 深层特征（layer3/4）：包含语义、形状等高级信息
- 融合两者可以检测不同类型的缺陷

```python
# ============================================================================
# 多尺度特征提取
# ============================================================================
# 使用 PyTorch 的 forward hook 机制从中间层提取特征
# 这是 PatchCore 等算法的核心技术之一
# ============================================================================

import torch
import torch.nn as nn
from typing import Dict, List
from torchvision import models


def extract_multiscale_features(
    model: nn.Module, 
    image: torch.Tensor, 
    layers: List[str] = ['layer2', 'layer3']
) -> Dict[str, torch.Tensor]:
    """
    使用 forward hook 从 CNN 中间层提取多尺度特征
    
    【Forward Hook 原理】
    PyTorch 的 hook 机制允许我们在不修改模型代码的情况下，
    截取任意中间层的输出。当前向传播经过某个层时，
    注册的 hook 函数会被自动调用。
    
    Args:
        model: 预训练的 CNN 模型（如 ResNet18）
               必须已设置为 eval 模式
        image: 输入图像 tensor
               形状: [B, C, H, W]，已经过归一化
        layers: 要提取特征的层名称列表
                ResNet 的层名称: 'layer1', 'layer2', 'layer3', 'layer4'
    
    Returns:
        字典，key 为层名称，value 为该层的输出特征
        特征形状: [B, C, H', W']，其中 H', W' 取决于层深度
    
    示例:
        >>> model = models.resnet18(pretrained=True).eval()
        >>> image = torch.randn(1, 3, 512, 512)
        >>> features = extract_multiscale_features(model, image, ['layer2', 'layer3'])
        >>> print(features['layer2'].shape)  # [1, 128, 64, 64]
        >>> print(features['layer3'].shape)  # [1, 256, 32, 32]
    """
    
    # 存储提取的特征
    features = {}
    
    # 创建 hook 函数的工厂函数
    # 为什么需要工厂函数？因为我们需要捕获 'name' 变量
    # 如果直接用 lambda，所有 hook 都会共享同一个 name
    def hook_fn(name: str):
        """
        创建一个 hook 函数，用于捕获指定层的输出
        
        Args:
            name: 层名称，用于存储特征时的键
        
        Returns:
            hook 函数，签名为 (module, input, output)
        """
        def hook(module, input, output):
            # module: 当前层的模块
            # input: 层的输入（tuple）
            # output: 层的输出（我们需要的特征）
            features[name] = output.detach()  # detach 防止占用计算图内存
        return hook
    
    # ====== 第一步：注册 hooks ======
    hooks = []  # 保存 hook 句柄，便于之后移除
    
    # 遍历模型的所有子模块
    for name, module in model.named_modules():
        # named_modules() 返回 (name, module) 对
        # name 是模块的层次路径，如 'layer2.0.conv1'
        
        if name in layers:
            # register_forward_hook 返回一个句柄（handle）
            # 可用于后续移除 hook
            handle = module.register_forward_hook(hook_fn(name))
            hooks.append(handle)
            print(f"[extract_features] 注册 hook: {name}")
    
    # ====== 第二步：前向传播 ======
    # 使用 torch.no_grad() 节省内存（推理时不需要梯度）
    with torch.no_grad():
        # 前向传播，hooks 会自动捕获中间特征
        _ = model(image)  # 不需要最终输出
    
    # ====== 第三步：移除 hooks ======
    # 重要！必须移除 hook，否则会影响后续的前向传播
    for hook in hooks:
        hook.remove()
    
    return features


# ============================================================================
# 【如何嵌入代码 - 完整示例】
# ============================================================================

"""
# 示例：使用预训练 ResNet 提取特征
# ----------------------------------------------------------------------------

import torch
from torchvision import models, transforms
from PIL import Image

# 1. 加载预训练模型
model = models.resnet18(pretrained=True)
model.eval()  # 必须设置为评估模式！

# 2. 准备图像预处理（ImageNet 标准）
preprocess = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet 均值
        std=[0.229, 0.224, 0.225]    # ImageNet 标准差
    ),
])

# 3. 加载并预处理图像
image = Image.open("test.png").convert("RGB")
image_tensor = preprocess(image).unsqueeze(0)  # 添加 batch 维度

# 4. 提取多尺度特征
features = extract_multiscale_features(
    model, 
    image_tensor, 
    layers=['layer2', 'layer3']
)

# 5. 查看特征形状
for name, feat in features.items():
    print(f"{name}: {feat.shape}")
    # layer2: torch.Size([1, 128, 64, 64])  - 较大分辨率，局部细节
    # layer3: torch.Size([1, 256, 32, 32])  - 较小分辨率，语义信息

# ----------------------------------------------------------------------------

# 在 PatchCore 中的应用：
# Anomalib 内部已经实现了类似的特征提取逻辑
# 通过配置 layers 参数即可：

from anomalib.models.patchcore import Patchcore

model = Patchcore(
    input_size=(512, 512),
    backbone="resnet18",
    layers=["layer2", "layer3"],  # 这里指定要提取的层
    pre_trained=True,
    coreset_sampling_ratio=0.1,
    num_neighbors=9,
)
"""
```

#### 3.2.2 特征金字塔网络

```
          Layer4 (深层语义)
              ↓
    Layer3 + Upsample(Layer4)
              ↓
    Layer2 + Upsample(上层融合)
              ↓
        融合特征输出
```

### 3.3 记忆库优化策略

#### 3.3.1 核心集采样优化

**目的**：从大量特征向量中选取具有代表性的子集，减少记忆库大小和推理时间。

**原理**：使用贪心 K-Center 算法，选择能够覆盖整个特征空间的代表性点。

**在 PatchCore 中的作用**：
- 原始特征可能有几十万个特征向量
- 通过 Coreset 采样压缩到 1-10%
- 减少内存占用，加快推理速度

```python
# ============================================================================
# 自适应核心集采样 (Adaptive Coreset Subsampling)
# ============================================================================
# 论文: "Towards Total Recall in Industrial Anomaly Detection" (CVPR 2022)
# 
# 核心集采样的目标：
# - 从 N 个特征点中选择 K 个最具代表性的点 (K << N)
# - 使得任意点到最近核心集点的距离最小化
# - 等价于 K-Center 问题，使用贪心算法近似求解
# ============================================================================

import numpy as np
from typing import Optional


class AdaptiveCoreset:
    """
    自适应核心集采样器
    
    根据数据量自动调整采样比例，并使用贪心 K-Center 算法
    选择最具代表性的特征子集。
    
    使用示例:
    ---------
    >>> coreset = AdaptiveCoreset(target_size_ratio=0.01, min_samples=100)
    >>> features = np.random.randn(10000, 512)  # 10000个512维特征
    >>> core_features = coreset.subsample(features)
    >>> print(core_features.shape)  # (100, 512) 或 (10000*0.01, 512)
    
    参数:
    -----
    target_size_ratio : float
        目标采样比例，默认 0.01 (1%)
        较小值: 更少内存，更快推理，可能损失精度
        较大值: 更高精度，但内存和时间成本增加
    min_samples : int
        最小采样数量，防止数据量小时采样太少
    """
    
    def __init__(self, target_size_ratio: float = 0.01, min_samples: int = 100):
        """
        初始化采样器
        
        Args:
            target_size_ratio: 目标采样比例
                - 0.01 = 1%：适合大数据量，节省内存
                - 0.1 = 10%：适合小数据量，保留更多信息
            min_samples: 最小采样数量
                - 保证即使数据量很小，也有足够的样本
        """
        self.target_size_ratio = target_size_ratio
        self.min_samples = min_samples
    
    def subsample(self, features: np.ndarray) -> np.ndarray:
        """
        根据特征分布自适应采样
        
        Args:
            features: 特征矩阵，形状为 [N, D]
                      N = 特征数量，D = 特征维度
        
        Returns:
            采样后的核心集，形状为 [K, D]，K <= N
        """
        n_samples = features.shape[0]
        
        # 计算目标采样数量
        target_size = max(
            int(n_samples * self.target_size_ratio),  # 按比例计算
            self.min_samples                           # 不低于最小值
        )
        
        # 如果目标数量大于等于原始数量，直接返回
        if target_size >= n_samples:
            print(f"[Coreset] 数据量较小 ({n_samples})，不需要采样")
            return features
        
        print(f"[Coreset] 从 {n_samples} 个点采样到 {target_size} 个点 "
              f"({target_size/n_samples*100:.2f}%)")
        
        # 使用 k-center 贪心算法采样
        selected_indices = self._greedy_kcenter(features, target_size)
        
        return features[selected_indices]
    
    def _greedy_kcenter(self, features: np.ndarray, k: int) -> np.ndarray:
        """
        贪心 K-Center 采样算法
        
        【算法原理】
        1. 随机选择第一个点作为起始核心点
        2. 计算所有点到已选核心点的最小距离
        3. 选择距离最大的点作为新的核心点
        4. 重复步骤 2-3 直到选择 k 个点
        
        这样可以保证核心集尽可能均匀地覆盖整个特征空间
        
        【时间复杂度】
        O(k * N)，其中 N 是数据点数量
        
        Args:
            features: 特征矩阵 [N, D]
            k: 要选择的点数
        
        Returns:
            被选中点的索引数组
        """
        n = features.shape[0]
        
        # 随机选择第一个点
        selected = [np.random.randint(n)]
        
        # 初始化每个点到已选集合的最小距离
        # 初始为无穷大，会在第一轮迭代中更新
        min_distances = np.full(n, np.inf)
        
        # 迭代选择剩余 k-1 个点
        for i in range(k - 1):
            # 获取最后一个被选中的点
            last_selected = features[selected[-1]]
            
            # 计算所有点到该点的距离
            # 使用欧几里得距离 (L2 范数)
            distances = np.linalg.norm(features - last_selected, axis=1)
            
            # 更新每个点到已选集合的最小距离
            # 取当前最小距离和新距离中的较小值
            min_distances = np.minimum(min_distances, distances)
            
            # 选择最小距离最大的点（即离已选集合最远的点）
            # 这保证了核心集的分布均匀性
            next_idx = np.argmax(min_distances)
            selected.append(next_idx)
            
            # 进度提示（每 10% 输出一次）
            if (i + 1) % (k // 10 + 1) == 0:
                print(f"[Coreset] 采样进度: {i+1}/{k-1}")
        
        return np.array(selected)


# ============================================================================
# 【如何嵌入代码】
# ============================================================================

"""
# Anomalib 内部已实现 Coreset 采样，通过参数配置即可：
# ----------------------------------------------------------------------------

from anomalib.models.patchcore import Patchcore

# 小样本场景推荐配置
model = Patchcore(
    input_size=(512, 512),
    backbone="resnet18",
    layers=["layer2", "layer3"],
    pre_trained=True,
    
    # 核心集采样比例 - 关键参数
    coreset_sampling_ratio=0.1,  # 小样本时用较大比例 (10%)
                                  # 大样本时可用 0.01 (1%)
    
    num_neighbors=9,  # K近邻数量
)

# ----------------------------------------------------------------------------

# 如果需要自定义采样逻辑，可以在训练后处理：
# ----------------------------------------------------------------------------

import torch
from anomalib.models.patchcore import Patchcore

# 训练后获取记忆库
model = ...  # 训练完成的模型

# 访问记忆库（Anomalib 内部属性）
memory_bank = model.model.memory_bank  # tensor: [N, D]

# 如果需要进一步压缩，可以应用自定义 Coreset
coreset = AdaptiveCoreset(target_size_ratio=0.05)
compressed_memory = coreset.subsample(memory_bank.cpu().numpy())
model.model.memory_bank = torch.from_numpy(compressed_memory).to(model.device)
"""
```

---

## 4. 领域自适应与分布偏移处理

### 4.1 产品变型问题分析

```
┌─────────────────────────────────────────────────┐
│              产品变型导致的分布偏移               │
├─────────────────────────────────────────────────┤
│                                                 │
│   批次A (训练)          批次B (测试)             │
│   ┌─────────┐          ┌─────────┐             │
│   │ ● ● ●   │          │   ○ ○ ○ │             │
│   │  ● ●    │    →     │ ○ ○ ○   │             │
│   │   ●     │          │  ○ ○    │             │
│   └─────────┘          └─────────┘             │
│                                                 │
│   特征分布中心发生偏移，导致误检率升高            │
│                                                 │
└─────────────────────────────────────────────────┘
```

### 4.2 域自适应策略

#### 4.2.1 特征归一化 (Feature Normalization)

**目的**：通过归一化减少不同批次产品之间的特征分布差异。

**原理**：将特征归一化到标准分布，使模型对光照、颜色等全局变化更不敏感。

**应用场景**：
- 不同光源色温导致的颜色偏差
- 不同时段光照强度变化
- 相机曝光参数波动

```python
# ============================================================================
# 特征归一化器 (Feature Normalizer)
# ============================================================================
# 用于减轻域偏移 (Domain Shift) 的影响
# 
# 域偏移问题：
# - 训练时：拍摄于上午 10点，自然光照
# - 测试时：拍摄于晚上 8点，人工照明
# - 导致：特征分布发生变化，模型误检率上升
# 
# 解决方案：归一化消除全局特征差异
# ============================================================================

import torch
import torch.nn as nn
from typing import Literal


class FeatureNormalizer:
    """
    特征归一化器，用于减轻域偏移影响
    
    支持多种归一化方式：
    - instance: 实例归一化，每张图像独立归一化
    - layer: 层归一化，跨通道归一化
    - batch: 批归一化，跨批次归一化（需维护统计量）
    
    使用示例:
    ---------
    >>> normalizer = FeatureNormalizer(method='instance')
    >>> features = torch.randn(2, 256, 64, 64)  # [B, C, H, W]
    >>> normalized = normalizer.normalize(features)
    >>> print(normalized.mean(), normalized.std())  # 近似 0, 1
    """
    
    def __init__(self, method: Literal['instance', 'layer', 'batch', 'none'] = 'instance'):
        """
        初始化特征归一化器
        
        Args:
            method: 归一化方法
                - 'instance': 实例归一化
                    在 H、W 维度上计算均值和标准差
                    每个样本、每个通道独立归一化
                    适合：去除光照/对比度差异
                    
                - 'layer': 层归一化
                    在 C、H、W 维度上计算均值和标准差
                    每个样本独立归一化
                    适合：去除更强的全局差异
                    
                - 'batch': 批归一化（需要训练阶段维护统计量）
                    在 B、H、W 维度上计算均值和标准差
                    跨批次共享统计量
                    
                - 'none': 不归一化
        """
        self.method = method
        
        # 用于 batch 归一化的运行统计量
        self.running_mean = None
        self.running_std = None
        self.momentum = 0.1  # 统计量更新动量
    
    def normalize(self, features: torch.Tensor) -> torch.Tensor:
        """
        对特征进行归一化
        
        归一化公式：
        normalized = (features - mean) / (std + eps)
        
        Args:
            features: 特征图，形状 [B, C, H, W]
                B = 批次大小
                C = 特征通道数
                H, W = 特征图高度、宽度
        
        Returns:
            归一化后的特征，形状不变
        
        示例:
            实例归一化前: mean=120.5, std=45.3
            实例归一化后: mean≈0.0, std≈1.0
        """
        # 小常数，防止除以零
        eps = 1e-8
        
        if self.method == 'instance':
            # ====== 实例归一化 ======
            # 在 H, W 维度上计算统计量
            # 每个样本的每个通道独立处理
            # 
            # 优点：每张图像独立，不依赖 batch 统计量
            # 缺点：可能丢失一些有用的全局信息
            
            # keepdim=True 保留维度，便于广播
            mean = features.mean(dim=[2, 3], keepdim=True)  # [B, C, 1, 1]
            std = features.std(dim=[2, 3], keepdim=True) + eps  # [B, C, 1, 1]
            
            return (features - mean) / std
        
        elif self.method == 'layer':
            # ====== 层归一化 ======
            # 在 C, H, W 维度上计算统计量
            # 每个样本独立处理，但所有通道共享
            # 
            # 优点：归一化效果更强
            # 缺点：可能破坏通道间的相对关系
            
            mean = features.mean(dim=[1, 2, 3], keepdim=True)  # [B, 1, 1, 1]
            std = features.std(dim=[1, 2, 3], keepdim=True) + eps
            
            return (features - mean) / std
        
        elif self.method == 'batch':
            # ====== 批归一化 ======
            # 使用运行统计量（类似 BatchNorm）
            # 需要在训练时更新统计量
            
            if self.training:
                # 计算当前 batch 的统计量
                batch_mean = features.mean(dim=[0, 2, 3], keepdim=True)
                batch_std = features.std(dim=[0, 2, 3], keepdim=True) + eps
                
                # 更新运行统计量
                if self.running_mean is None:
                    self.running_mean = batch_mean
                    self.running_std = batch_std
                else:
                    self.running_mean = (1 - self.momentum) * self.running_mean + \
                                         self.momentum * batch_mean
                    self.running_std = (1 - self.momentum) * self.running_std + \
                                        self.momentum * batch_std
                
                return (features - batch_mean) / batch_std
            else:
                # 推理时使用运行统计量
                return (features - self.running_mean) / self.running_std
        
        # method == 'none'
        return features
    
    def train(self):
        """切换到训练模式"""
        self.training = True
    
    def eval(self):
        """切换到评估模式"""
        self.training = False


# ============================================================================
# 【如何嵌入代码】
# ============================================================================

"""
# 方法1：作为特征提取的后处理
# ----------------------------------------------------------------------------

import torch
from torchvision import models

# 加载模型
backbone = models.resnet18(pretrained=True).eval()
normalizer = FeatureNormalizer(method='instance')

def extract_normalized_features(image):
    # 提取特征
    with torch.no_grad():
        features = extract_multiscale_features(backbone, image, ['layer2', 'layer3'])
    
    # 对每个层的特征进行归一化
    normalized_features = {}
    for name, feat in features.items():
        normalized_features[name] = normalizer.normalize(feat)
    
    return normalized_features

# ----------------------------------------------------------------------------

# 方法2：在 PatchCore 训练/推理中使用
# Anomalib 内置了特征归一化，无需额外配置
# 但如果需要自定义，可以通过替换特征提取器实现
# ----------------------------------------------------------------------------
"""
```

#### 4.2.2 风格迁移增强

```python
def adaptive_instance_normalization(content_feat, style_feat):
    """
    自适应实例归一化 (AdaIN)
    将训练样本的风格迁移到不同产品变型
    """
    # 计算内容特征的统计量
    content_mean = content_feat.mean(dim=[2, 3], keepdim=True)
    content_std = content_feat.std(dim=[2, 3], keepdim=True) + 1e-8
    
    # 计算风格特征的统计量
    style_mean = style_feat.mean(dim=[2, 3], keepdim=True)
    style_std = style_feat.std(dim=[2, 3], keepdim=True) + 1e-8
    
    # 归一化后重新调整
    normalized = (content_feat - content_mean) / content_std
    return normalized * style_std + style_mean
```

#### 4.2.3 增量学习机制

**目的**：当产品发生变化时，无需重新训练，只需用新样本更新记忆库。

**应用场景**：
- 产品迭代升级，外观略有变化
- 新生产线上线，需要快速适应
- 季节性光照变化

**核心思想**：将新的正常样本特征添加到记忆库，同时维护合理的记忆库大小。

```python
# ============================================================================
# 支持增量学习的记忆库 (Incremental Memory Bank)
# ============================================================================
# 工业场景中，产品经常迭代更新。
# 增量学习允许模型在不重新训练的情况下适应新产品变型。
# 
# 工作流程：
# 1. 初始训练: 使用批次A的正常样本建立记忆库
# 2. 产品变型: 批次B上线，外观略有不同
# 3. 增量更新: 使用批次B的几个正常样本更新记忆库
# 4. 继续检测: 无需重新训练，模型即可适应新批次
# ============================================================================

import torch
import numpy as np
from typing import Optional


class IncrementalMemoryBank:
    """
    支持增量学习的记忆库
    
    允许在不重新训练模型的情况下，通过添加新样本来适应
    产品变型或环境变化。
    
    特点：
    - 自动维护记忆库大小，防止无限增长
    - 智能筛选新样本，只添加有代表性的特征
    - 使用 Coreset 采样保持记忆库的代表性
    
    使用示例:
    ---------
    >>> memory_bank = IncrementalMemoryBank(max_size=10000, update_ratio=0.1)
    >>> # 初始化（用训练数据）
    >>> initial_features = extract_features(training_images)
    >>> memory_bank.initialize(initial_features)
    >>> # 增量更新（用新批次数据）
    >>> new_features = extract_features(new_batch_images)
    >>> memory_bank.update(new_features)
    
    参数:
    -----
    max_size : int
        记忆库最大容量（特征向量数量）
    update_ratio : float
        每次更新时添加的新样本比例
    """
    
    def __init__(self, max_size: int = 10000, update_ratio: float = 0.1):
        """
        初始化增量记忆库
        
        Args:
            max_size: 记忆库最大大小
                - 较大值 (50000): 更高精度，更多内存
                - 较小值 (5000): 更快推理，更少内存
            update_ratio: 更新比例
                - 0.1 = 10%: 保守更新，适合稳定场景
                - 0.3 = 30%: 激进更新，适合变化较大的场景
        """
        self.memory = None          # 记忆库 tensor: [N, D]
        self.max_size = max_size
        self.update_ratio = update_ratio
        
        # 记录更新历史（可选，用于调试）
        self.update_history = []
    
    def initialize(self, features: torch.Tensor):
        """
        用初始特征初始化记忆库
        
        这通常在模型训练完成后调用，使用训练集提取的特征。
        
        Args:
            features: 初始特征，形状 [N, D]
                      N = 特征数量，D = 特征维度
        """
        print(f"[IncrementalMemoryBank] 初始化记忆库，输入特征数: {features.shape[0]}")
        
        self.memory = features.clone()
        self._maintain_size()  # 确保不超过最大容量
        
        print(f"[IncrementalMemoryBank] 初始化完成，记忆库大小: {self.memory.shape[0]}")
    
    def update(self, new_features: torch.Tensor):
        """
        增量更新记忆库
        
        用于适应新产品变型或环境变化。
        
        更新策略：
        1. 计算新特征与现有记忆库的相似度
        2. 选择与现有记忆差异最大的新特征（最具代表性）
        3. 将选中的新特征添加到记忆库
        4. 如果超过最大容量，使用 Coreset 采样压缩
        
        Args:
            new_features: 新样本的特征，形状 [M, D]
        """
        # 如果记忆库为空，直接初始化
        if self.memory is None:
            self.initialize(new_features)
            return
        
        print(f"[IncrementalMemoryBank] 开始增量更新，新特征数: {new_features.shape[0]}")
        
        # Step 1: 计算新样本与记忆库的相似度（最近邻距离）
        # 距离越大，说明该特征与现有记忆越不同，越应该被添加
        similarities = self._compute_similarity(new_features)
        
        # Step 2: 选择代表性新样本（与现有记忆差异大的）
        # 根据 update_ratio 计算要添加的数量
        n_update = int(len(new_features) * self.update_ratio)
        n_update = max(n_update, 1)  # 至少添加 1 个
        
        # 选择距离最大的 n_update 个特征
        # argsort 返回从小到大的索引，取后 n_update 个（即最大的）
        update_indices = similarities.argsort(descending=True)[:n_update]
        
        print(f"[IncrementalMemoryBank] 选择了 {n_update} 个代表性特征进行更新")
        
        # Step 3: 更新记忆库
        self.memory = torch.cat([
            self.memory,
            new_features[update_indices]
        ], dim=0)
        
        # Step 4: 维护记忆库大小
        self._maintain_size()
        
        # 记录更新历史
        self.update_history.append({
            'n_new_features': new_features.shape[0],
            'n_added': n_update,
            'memory_size_after': self.memory.shape[0]
        })
        
        print(f"[IncrementalMemoryBank] 更新完成，当前记忆库大小: {self.memory.shape[0]}")
    
    def _maintain_size(self):
        """
        维护记忆库大小不超过最大容量
        
        如果超过最大容量，使用 Coreset 采样压缩记忆库，
        保留最具代表性的特征。
        """
        if self.memory is None:
            return
        
        if len(self.memory) > self.max_size:
            print(f"[IncrementalMemoryBank] 记忆库超过最大容量 "
                  f"({len(self.memory)} > {self.max_size})，执行 Coreset 采样")
            
            # 使用核心集采样减少大小
            indices = self._coreset_subsample(self.max_size)
            self.memory = self.memory[indices]
            
            print(f"[IncrementalMemoryBank] 采样完成，新大小: {len(self.memory)}")
    
    def _compute_similarity(self, features: torch.Tensor) -> torch.Tensor:
        """
        计算特征与记忆库的相似度
        
        使用最近邻距离作为相似度度量：
        - 距离越小，相似度越高（与现有记忆重复）
        - 距离越大，相似度越低（新的信息，应该添加）
        
        Args:
            features: 要计算的特征 [M, D]
        
        Returns:
            每个特征的最近邻距离 [M]
        """
        # torch.cdist 计算所有特征对之间的距离
        # 输出形状: [M, N]，其中 M 是新特征数，N 是记忆库大小
        distances = torch.cdist(features, self.memory)
        
        # 取每个新特征到记忆库的最小距离
        min_distances, _ = distances.min(dim=1)
        
        return min_distances
    
    def _coreset_subsample(self, target_size: int) -> np.ndarray:
        """
        核心集子采样
        
        使用贪心 K-Center 算法选择最具代表性的特征子集。
        
        Args:
            target_size: 目标采样数量
        
        Returns:
            被选中点的索引
        """
        # 转换为 numpy 进行计算
        features_np = self.memory.cpu().numpy()
        n = features_np.shape[0]
        
        # 贪心 K-Center 采样
        selected = [np.random.randint(n)]
        min_distances = np.full(n, np.inf)
        
        for _ in range(target_size - 1):
            last_selected = features_np[selected[-1]]
            distances = np.linalg.norm(features_np - last_selected, axis=1)
            min_distances = np.minimum(min_distances, distances)
            next_idx = np.argmax(min_distances)
            selected.append(next_idx)
        
        return np.array(selected)
    
    def query(self, features: torch.Tensor, k: int = 9) -> tuple:
        """
        查询特征的 K 近邻
        
        用于计算异常分数。
        
        Args:
            features: 要查询的特征 [M, D]
            k: 近邻数量
        
        Returns:
            distances: K 近邻距离 [M, k]
            indices: K 近邻索引 [M, k]
        """
        distances = torch.cdist(features, self.memory)
        knn_distances, knn_indices = distances.topk(k, largest=False)
        return knn_distances, knn_indices


# ============================================================================
# 【如何嵌入代码 - 完整工作流示例】
# ============================================================================

"""
# 完整的增量学习工作流
# ----------------------------------------------------------------------------

import torch
from anomalib.models.patchcore import Patchcore
from anomalib.data.folder import Folder

# ===== 第一阶段：初始训练 =====
print("第一阶段：使用批次A数据训练")

# 创建并训练模型…
# （正常训练流程）

# ===== 第二阶段：制造增量记忆库 =====
print("第二阶段：初始化增量记忆库")

# 从训练好的模型获取记忆库
model = ...  # 训练完成的模型
initial_memory = model.model.memory_bank

# 创建增量记忆库
incremental_memory = IncrementalMemoryBank(
    max_size=10000,    # 最大容量
    update_ratio=0.1   # 每次添加 10% 的新特征
)
incremental_memory.initialize(initial_memory)

# ===== 第三阶段：产品变型，增量更新 =====
print("第三阶段：产品变型，使用新样本更新")

# 加载新批次的几个正常样本
new_batch_images = load_images("./new_batch/normal/")  # 只需要几张

# 提取特征
new_features = []
for img in new_batch_images:
    feat = model.model.extract_features(preprocess(img))
    new_features.append(feat)
new_features = torch.cat(new_features, dim=0)

# 增量更新
incremental_memory.update(new_features)

# 更新模型的记忆库
model.model.memory_bank = incremental_memory.memory

# ===== 第四阶段：继续检测 =====
print("第四阶段：使用更新后的模型检测")

# 现在模型可以适应新批次的产品
result = model.predict(test_image)
print(f"异常分数: {result['anomaly_score']}")
"""
```

### 4.3 在线自适应推理

```python
class AdaptiveInference:
    """自适应推理引擎"""
    
    def __init__(self, model, memory_bank, adaptation_threshold=0.3):
        self.model = model
        self.memory_bank = memory_bank
        self.adaptation_threshold = adaptation_threshold
        self.running_stats = RunningStatistics()
    
    def predict(self, image, adapt=True):
        """
        自适应推理
        
        Args:
            image: 输入图像
            adapt: 是否启用在线自适应
        """
        # 特征提取
        features = self.model.extract_features(image)
        
        # 计算异常分数
        score, anomaly_map = self._compute_anomaly_score(features)
        
        # 在线自适应
        if adapt and self._is_normal(score):
            # 如果检测为正常，可用于更新记忆库
            self.memory_bank.update(features)
        
        # 自适应阈值
        threshold = self._adaptive_threshold(score)
        
        return {
            'score': score,
            'anomaly_map': anomaly_map,
            'threshold': threshold,
            'is_anomaly': score > threshold
        }
    
    def _is_normal(self, score):
        """判断是否为正常样本（用于在线学习）"""
        return score < self.adaptation_threshold
    
    def _adaptive_threshold(self, score):
        """自适应阈值计算"""
        self.running_stats.update(score)
        
        # 基于运行统计量动态调整阈值
        mean = self.running_stats.mean
        std = self.running_stats.std
        
        return mean + 3 * std  # 3-sigma 原则
```

---

## 5. 可解释性与自适应推理机制

### 5.1 异常热力图可视化

#### 5.1.1 热力图生成原理

**目的**：将模型的异常检测结果可视化，便于人工审核和质量追溯。

**热力图显示**：
- 红色区域：异常分数高，可能存在缺陷
- 蓝色区域：异常分数低，正常区域
- 透明度：控制热力图与原图的融合程度

```python
# ============================================================================
# 异常热力图生成器
# ============================================================================
# 将模型输出的异常分数图转换为直观的热力图可视化
# 
# 应用场景：
# - 生产线工人查看检测结果
# - 质量审核报告
# - 模型调试和分析
# ============================================================================

import cv2
import numpy as np
from typing import Optional, Tuple


def generate_anomaly_heatmap(
    anomaly_map: np.ndarray,
    image: np.ndarray,
    alpha: float = 0.5,
    colormap: int = cv2.COLORMAP_JET,
    normalize: bool = True
) -> np.ndarray:
    """
    生成异常热力图可视化
    
    将异常分数图转换为彩色热力图，并叠加到原图上。
    
    处理流程：
    1. 归一化异常分数到 [0, 1] 范围
    2. 应用颜色映射（JET colormap）
    3. 与原图进行 alpha 混合
    
    Args:
        anomaly_map: 异常分数图，形状 [H, W]
            值越大表示异常程度越高
        image: 原始图像，形状 [H, W, 3]
            必须是 BGR 格式（OpenCV 默认）或 RGB 格式
        alpha: 叠加透明度，范围 [0, 1]
            0.0 = 只显示原图
            0.5 = 半透明叠加（默认）
            1.0 = 只显示热力图
        colormap: OpenCV 颜色映射方案
            cv2.COLORMAP_JET - 常用，蓝色到红色
            cv2.COLORMAP_HOT - 黑色到红色到白色
            cv2.COLORMAP_INFERNO - 黑色到黄色
        normalize: 是否对异常分数进行归一化
    
    Returns:
        overlay: 叠加热力图后的可视化图像 [H, W, 3]
    
    示例:
        >>> # 模型推理
        >>> result = model.predict(image)
        >>> anomaly_map = result['anomaly_map']  # [H, W]
        >>> 
        >>> # 生成热力图
        >>> heatmap = generate_anomaly_heatmap(anomaly_map, image, alpha=0.5)
        >>> 
        >>> # 保存或显示
        >>> cv2.imwrite('heatmap.png', heatmap)
    """
    
    # ====== Step 1: 归一化异常分数 ======
    if normalize:
        # 将异常分数归一化到 [0, 1] 范围
        # 使用 min-max 归一化
        min_val = anomaly_map.min()
        max_val = anomaly_map.max()
        
        # 防止除以零
        if max_val - min_val > 1e-8:
            heatmap = (anomaly_map - min_val) / (max_val - min_val)
        else:
            # 如果所有值都相同，设为 0
            heatmap = np.zeros_like(anomaly_map)
    else:
        # 假设输入已经在 [0, 1] 范围
        heatmap = np.clip(anomaly_map, 0, 1)
    
    # ====== Step 2: 转换为 8 位无符号整数 ======
    # OpenCV 的 applyColorMap 需要 uint8 输入
    heatmap_uint8 = (heatmap * 255).astype(np.uint8)
    
    # ====== Step 3: 应用颜色映射 ======
    # JET colormap: 蓝(0) -> 青 -> 黄 -> 红(255)
    # 低异常分数 -> 蓝色/冷色调
    # 高异常分数 -> 红色/暖色调
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, colormap)
    # 输出形状: [H, W, 3]  BGR 格式
    
    # ====== Step 4: 与原图 alpha 混合 ======
    # 确保原图为 uint8 格式
    if image.dtype != np.uint8:
        image = (image * 255).astype(np.uint8) if image.max() <= 1 else image.astype(np.uint8)
    
    # 确保原图尺寸与热力图一致
    if image.shape[:2] != heatmap_colored.shape[:2]:
        heatmap_colored = cv2.resize(
            heatmap_colored, 
            (image.shape[1], image.shape[0]),
            interpolation=cv2.INTER_LINEAR
        )
    
    # alpha 混合
    # overlay = (1 - alpha) * image + alpha * heatmap
    overlay = cv2.addWeighted(
        image,           # 背景图像
        1 - alpha,       # 背景权重
        heatmap_colored, # 前景图像（热力图）
        alpha,           # 前景权重
        0                # 添加到结果的标量
    )
    
    return overlay


# ============================================================================
# 高级热力图生成（带边界棆和标注）
# ============================================================================

def generate_detailed_heatmap(
    anomaly_map: np.ndarray,
    image: np.ndarray,
    threshold: float = 0.5,
    alpha: float = 0.5,
    show_bbox: bool = True,
    show_contour: bool = True
) -> Tuple[np.ndarray, list]:
    """
    生成详细的异常热力图，包含异常区域标注
    
    Args:
        anomaly_map: 异常分数图 [H, W]
        image: 原始图像 [H, W, 3]
        threshold: 异常判定阈值，用于绘制边界
        alpha: 热力图透明度
        show_bbox: 是否绘制边界框
        show_contour: 是否绘制轮廓
    
    Returns:
        overlay: 叠加后的可视化图像
        regions: 检测到的异常区域信息列表
    """
    # 基础热力图
    overlay = generate_anomaly_heatmap(anomaly_map, image, alpha)
    
    # 归一化异常图用于阈值处理
    norm_map = (anomaly_map - anomaly_map.min()) / (anomaly_map.max() - anomaly_map.min() + 1e-8)
    
    # 二值化
    binary = (norm_map > threshold).astype(np.uint8) * 255
    
    # 找轮廓
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    regions = []
    for i, contour in enumerate(contours):
        # 过滤太小的轮廓
        area = cv2.contourArea(contour)
        if area < 100:  # 小于 100 像素的区域忽略
            continue
        
        # 获取边界框
        x, y, w, h = cv2.boundingRect(contour)
        
        # 计算区域内的最大异常分数
        region_score = norm_map[y:y+h, x:x+w].max()
        
        regions.append({
            'id': i + 1,
            'bbox': (x, y, w, h),
            'area': area,
            'score': float(region_score),
            'center': (x + w // 2, y + h // 2)
        })
        
        # 绘制边界框
        if show_bbox:
            cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), 2)
            # 添加标签
            label = f"#{i+1} ({region_score:.2f})"
            cv2.putText(overlay, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # 绘制轮廓
        if show_contour:
            cv2.drawContours(overlay, [contour], -1, (255, 0, 0), 2)
    
    return overlay, regions


# ============================================================================
# 【如何嵌入代码】
# ============================================================================

"""
# 与 Anomalib 结合使用
# ----------------------------------------------------------------------------

from anomalib.models.patchcore import Patchcore
from anomalib.data.inference import InferenceDataset
import cv2

# 加载训练好的模型
model = Patchcore.load_from_checkpoint('results/task_1/weights/model.ckpt')
model.eval()

# 推理
image_path = 'test_image.png'
result = model.predict(image_path)

# 获取异常图
image = cv2.imread(image_path)
anomaly_map = result['anomaly_map'].cpu().numpy()  # [H, W]

# 生成热力图
heatmap = generate_anomaly_heatmap(anomaly_map, image, alpha=0.5)

# 保存
cv2.imwrite('result_heatmap.png', heatmap)

# 生成带标注的详细热力图
detailed_heatmap, regions = generate_detailed_heatmap(
    anomaly_map, image, threshold=0.5
)
print(f"检测到 {len(regions)} 个异常区域")
for r in regions:
    print(f"  区域 #{r['id']}: 分数={r['score']:.3f}, 面积={r['area']}")
"""
```

#### 5.1.2 多级别可视化

```
┌─────────────────────────────────────────────────────────────────┐
│                       多级别可解释性输出                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Level 1: 图像级别                                               │
│  ┌─────────────────┐                                            │
│  │ 判定: 异常       │  置信度: 0.87  阈值: 0.65                  │
│  └─────────────────┘                                            │
│                                                                 │
│  Level 2: 区域级别                                               │
│  ┌─────────────────┐                                            │
│  │  ╔════════╗    │  异常区域: (x=120, y=230, w=50, h=40)      │
│  │  ║ 异常区 ║    │  区域分数: 0.92                             │
│  │  ╚════════╝    │  缺陷类型推测: 划痕                         │
│  └─────────────────┘                                            │
│                                                                 │
│  Level 3: 像素级别                                               │
│  ┌─────────────────┐                                            │
│  │ [热力图可视化]   │  每个像素的异常分数                        │
│  └─────────────────┘                                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 注意力可视化

```python
class AttentionVisualizer:
    """注意力可视化器"""
    
    def __init__(self, model):
        self.model = model
        self.attention_maps = {}
    
    def register_hooks(self):
        """注册注意力层的钩子"""
        for name, module in self.model.named_modules():
            if 'attention' in name.lower():
                module.register_forward_hook(
                    self._create_hook(name)
                )
    
    def _create_hook(self, name):
        def hook(module, input, output):
            if isinstance(output, tuple):
                # 某些注意力层返回 (output, attention_weights)
                self.attention_maps[name] = output[1]
            else:
                self.attention_maps[name] = output
        return hook
    
    def visualize(self, image):
        """可视化注意力图"""
        self.attention_maps.clear()
        
        with torch.no_grad():
            _ = self.model(image)
        
        visualizations = {}
        for name, attn_map in self.attention_maps.items():
            # 处理注意力图
            attn = attn_map.mean(dim=1)  # 平均多头
            attn = F.interpolate(
                attn.unsqueeze(1),
                size=image.shape[-2:],
                mode='bilinear'
            ).squeeze()
            visualizations[name] = attn
        
        return visualizations
```

### 5.3 置信度评估

```python
class ConfidenceEstimator:
    """置信度估计器"""
    
    def __init__(self, calibration_method='temperature'):
        self.calibration_method = calibration_method
        self.temperature = 1.0
    
    def calibrate(self, validation_scores, validation_labels):
        """
        使用验证集校准置信度
        
        Args:
            validation_scores: 验证集异常分数
            validation_labels: 验证集真实标签
        """
        if self.calibration_method == 'temperature':
            # 温度缩放校准
            self.temperature = self._optimize_temperature(
                validation_scores, validation_labels
            )
    
    def estimate_confidence(self, anomaly_score, threshold):
        """
        估计预测置信度
        
        Args:
            anomaly_score: 异常分数
            threshold: 决策阈值
        
        Returns:
            置信度分数 (0-1)
        """
        # 计算到阈值的距离
        distance_to_threshold = abs(anomaly_score - threshold)
        
        # 使用 sigmoid 函数转换为置信度
        confidence = torch.sigmoid(
            torch.tensor(distance_to_threshold / self.temperature)
        )
        
        return confidence.item()
    
    def get_uncertainty(self, features, memory_bank, n_neighbors=5):
        """
        基于 K 近邻计算预测不确定性
        
        返回: 不确定性分数 (0-1)，越高表示越不确定
        """
        # 计算到最近邻的距离
        distances = torch.cdist(features, memory_bank)
        knn_distances, _ = distances.topk(n_neighbors, largest=False)
        
        # 距离的方差作为不确定性度量
        uncertainty = knn_distances.std(dim=1).mean()
        
        return uncertainty
```

### 5.4 决策可追溯性

```python
class DecisionExplainer:
    """决策解释器"""
    
    def __init__(self, model, memory_bank):
        self.model = model
        self.memory_bank = memory_bank
    
    def explain(self, image, anomaly_map):
        """
        生成决策解释报告
        
        Returns:
            explanation: 包含决策依据的字典
        """
        explanation = {
            'summary': '',
            'regions': [],
            'confidence': 0.0,
            'contributing_factors': [],
            'similar_normal_patches': []
        }
        
        # 1. 提取异常区域
        regions = self._extract_anomaly_regions(anomaly_map)
        explanation['regions'] = regions
        
        # 2. 分析每个异常区域
        for region in regions:
            factor = self._analyze_region(image, region)
            explanation['contributing_factors'].append(factor)
        
        # 3. 找到最相似的正常 patch（对比分析）
        similar_patches = self._find_similar_normal(image, regions)
        explanation['similar_normal_patches'] = similar_patches
        
        # 4. 生成文字摘要
        explanation['summary'] = self._generate_summary(explanation)
        
        return explanation
    
    def _extract_anomaly_regions(self, anomaly_map, threshold=0.5):
        """提取异常区域"""
        # 二值化
        binary = (anomaly_map > threshold).astype(np.uint8)
        
        # 连通域分析
        contours, _ = cv2.findContours(
            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        
        regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            area = cv2.contourArea(contour)
            score = anomaly_map[y:y+h, x:x+w].max()
            
            regions.append({
                'bbox': (x, y, w, h),
                'area': area,
                'score': score,
                'contour': contour
            })
        
        return sorted(regions, key=lambda r: r['score'], reverse=True)
    
    def _generate_summary(self, explanation):
        """生成文字摘要"""
        n_regions = len(explanation['regions'])
        if n_regions == 0:
            return "未检测到异常区域，判定为正常。"
        
        max_score = max(r['score'] for r in explanation['regions'])
        total_area = sum(r['area'] for r in explanation['regions'])
        
        return (
            f"检测到 {n_regions} 个异常区域，"
            f"最大异常分数 {max_score:.3f}，"
            f"总异常面积 {total_area} 像素。"
            f"置信度: {explanation['confidence']:.2%}"
        )
```

---

## 6. 系统实现细节

### 6.1 完整训练流程

**以下是将上述所有技术整合到一起的完整训练脚本示例：**

```python
#!/usr/bin/env python3
"""
完整训练流程示例 - 整合所有小样本学习技术

本脚本展示了如何将以下技术整合到训练流程中：
- 数据增强（几何+颜色）
- 多尺度特征提取
- 核心集采样
- 热力图可视化
"""

# ============================================================================
# 导入依赖
# ============================================================================
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

from pathlib import Path
import torch
import cv2
import numpy as np
import albumentations as A

# PyTorch Lightning
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

# Anomalib
from anomalib.data.folder import Folder
from anomalib.data.task_type import TaskType
from anomalib.models.patchcore import Patchcore
from anomalib.post_processing import NormalizationMethod, ThresholdMethod
from anomalib.utils.callbacks import (
    MetricsConfigurationCallback,
    PostProcessingConfigurationCallback,
)


def create_augmentation_pipeline():
    """
    创建数据增强管线
    
    小样本场景必须使用数据增强！
    """
    transform = A.Compose([
        # === 几何变换 ===
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.ShiftScaleRotate(
            shift_limit=0.05,
            scale_limit=0.1,
            rotate_limit=15,
            border_mode=cv2.BORDER_CONSTANT,
            p=0.5
        ),
        
        # === 颜色/光照变换 ===
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
        A.GaussianBlur(blur_limit=(3, 5), p=0.2),
    ])
    
    return transform


def create_model(image_size, sample_count):
    """
    创建 PatchCore 模型，根据样本数量调整参数
    
    Args:
        image_size: 图像尺寸
        sample_count: 训练样本数量（影响 coreset_sampling_ratio）
    """
    # 根据样本数量调整采样比例
    if sample_count < 50:
        # 极小样本: 保留更多特征
        coreset_ratio = 0.2
    elif sample_count < 200:
        # 小样本: 适中比例
        coreset_ratio = 0.1
    else:
        # 较大样本: 可以压缩更多
        coreset_ratio = 0.05
    
    model = Patchcore(
        input_size=image_size,
        backbone="resnet18",          # 小样本用轻量级主干
        layers=["layer2", "layer3"],  # 多尺度特征
        pre_trained=True,              # 必须使用预训练
        coreset_sampling_ratio=coreset_ratio,
        num_neighbors=9,
    )
    
    print(f"[模型配置] 样本数: {sample_count}, Coreset比例: {coreset_ratio}")
    
    return model


def main():
    """主训练流程"""
    
    # ========== 1. 配置 ==========
    dataset_root = Path("./datasets/task_1")
    output_dir = Path("./results/task_1")
    image_size = (512, 512)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ========== 2. 数据增强 ==========
    print("\n[Step 2] 创建数据增强管线...")
    train_transform = create_augmentation_pipeline()
    
    # ========== 3. 数据模块 ==========
    print("\n[Step 3] 初始化数据模块...")
    datamodule = Folder(
        normal_dir=dataset_root / "good",
        abnormal_dir=dataset_root / "defect",
        mask_dir=dataset_root / "mask" / "defect",
        image_size=image_size,
        train_batch_size=1,
        eval_batch_size=1,
        num_workers=8,
        task=TaskType.SEGMENTATION,
        transform_config_train=train_transform,
    )
    datamodule.setup()
    
    sample_count = len(datamodule.train_data)
    print(f"训练集: {sample_count} 张, 测试集: {len(datamodule.test_data)} 张")
    
    # ========== 4. 创建模型 ==========
    print("\n[Step 4] 创建模型...")
    model = create_model(image_size, sample_count)
    
    # ========== 5. 回调函数 ==========
    callbacks = [
        MetricsConfigurationCallback(
            task=TaskType.SEGMENTATION,
            image_metrics=["AUROC", "F1Score"],
            pixel_metrics=["AUROC", "F1Score"],
        ),
        PostProcessingConfigurationCallback(
            normalization_method=NormalizationMethod.MIN_MAX,
            threshold_method=ThresholdMethod.ADAPTIVE,
        ),
        ModelCheckpoint(
            dirpath=output_dir / "weights",
            filename="model",
            monitor="pixel_AUROC",
            mode="max",
            save_last=True,
        ),
    ]
    
    # ========== 6. 训练器 ==========
    print("\n[Step 6] 初始化训练器...")
    trainer = Trainer(
        max_epochs=1,
        accelerator="gpu",
        devices=1,
        default_root_dir=output_dir,
        logger=TensorBoardLogger(save_dir="logs/", name="task_1"),
        callbacks=callbacks,
        num_sanity_val_steps=0,
    )
    
    # ========== 7. 训练 ==========
    print("\n[Step 7] 开始训练...")
    trainer.fit(model=model, datamodule=datamodule)
    
    # ========== 8. 测试 ==========
    print("\n[Step 8] 测试评估...")
    test_results = trainer.test(model=model, datamodule=datamodule)
    
    print("\n" + "=" * 50)
    print("测试结果:")
    for key, value in test_results[0].items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    print("=" * 50)
    
    print(f"\n训练完成！模型保存于: {output_dir / 'weights'}")


if __name__ == "__main__":
    main()
```

详细代码请参见 [demo/advanced_train.py](../demo/advanced_train.py)

### 6.2 关键配置参数

#### 6.2.1 模型参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| `backbone` | `resnet18` | 小样本场景推荐轻量级主干 |
| `layers` | `["layer2", "layer3"]` | 多尺度特征提取 |
| `coreset_sampling_ratio` | `0.01-0.1` | 样本少时可适当增大 |
| `num_neighbors` | `9` | K近邻数量 |
| `input_size` | `(256, 256)` 或 `(512, 512)` | 根据缺陷大小调整 |

#### 6.2.2 数据参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| `train_batch_size` | `1-4` | 小样本场景推荐小批次 |
| `normal_split_ratio` | `0.2` | 20% 正常样本用于测试 |
| `augmentation` | 启用 | 必须启用数据增强 |

#### 6.2.3 后处理参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| `normalization_method` | `MIN_MAX` | 异常分数归一化 |
| `threshold_method` | `ADAPTIVE` | 自适应阈值 |

### 6.3 性能优化建议

#### 6.3.1 内存优化

```python
# 1. 减少 coreset 采样比例
model = Patchcore(
    coreset_sampling_ratio=0.01,  # 1% 采样
    ...
)

# 2. 使用混合精度
trainer = Trainer(
    precision=16,  # FP16 混合精度
    ...
)

# 3. 渐进式训练（大图像场景）
# 先用小尺寸建立记忆库，再用大尺寸微调
```

#### 6.3.2 速度优化

```python
# 1. 使用轻量级主干
model = Patchcore(
    backbone="resnet18",  # 而非 wide_resnet50_2
    ...
)

# 2. 减少特征层数量
model = Patchcore(
    layers=["layer2"],  # 单层特征
    ...
)

# 3. 启用 ONNX 导出加速推理
from anomalib.deploy import export_to_onnx
export_to_onnx(model, input_size=(1, 3, 256, 256), output_path="model.onnx")
```

---

## 7. 算法选型对比与推荐

### 7.1 各算法对比表

| 算法 | 训练时间 | 推理速度 | 精度 | 内存占用 | 小样本适应性 | 推荐场景 |
|------|----------|----------|------|----------|--------------|----------|
| **PatchCore** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | **首选推荐** |
| PaDiM | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 内存敏感 |
| STFPM | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 实时检测 |
| FastFlow | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 实时检测 |
| EfficientAd | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 边缘部署 |
| DRAEM | ⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 无缺陷样本 |
| CFlow | ⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | 追求极限精度 |

### 7.2 场景推荐

```
┌─────────────────────────────────────────────────────────────────┐
│                         算法选型决策树                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    正常样本数量 < 50?                            │
│                         │                                       │
│            ┌────────────┴────────────┐                         │
│           Yes                        No                         │
│            │                          │                         │
│      使用 PaDiM               缺陷样本是否充足?                   │
│     (统计方法更稳定)                  │                          │
│                          ┌───────────┴───────────┐              │
│                         Yes                      No              │
│                          │                        │              │
│                    考虑监督学习              使用 PatchCore       │
│                   (如有需要)                   (首选)            │
│                                                                 │
│                         需要实时推理?                            │
│                              │                                  │
│                  ┌───────────┴───────────┐                     │
│                 Yes                      No                     │
│                  │                        │                     │
│            EfficientAd               PatchCore                  │
│            或 FastFlow                                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.3 针对您数据集的推荐配置

根据您的 `datasets` 结构（每个任务约 100 张正常样本，100 张缺陷样本）：

```python
# ============================================================================
# 推荐配置 - 针对小样本工业缺陷检测场景
# ============================================================================
# 
# 数据集情况分析：
# - 正常样本: ~100 张（用于训练）
# - 缺陷样本: ~100 张（仅用于测试）
# - 属于典型的小样本场景
# 
# 配置原则：
# - 使用轻量级模型避免过拟合
# - 启用数据增强扩充训练数据
# - 使用较大的 coreset 比例保留更多信息
# ============================================================================

# ----- 完整配置字典 -----
recommended_config = {
    # ===== 模型配置 =====
    'model': 'PatchCore',       # 推荐算法：PatchCore
                                 # 原因：仅需正常样本，一次性学习，适合小样本
    
    'backbone': 'resnet18',      # 骨干网络：ResNet18
                                 # 原因：样本少时用轻量级主干避免过拟合
                                 # 其他选项：'resnet50'（更精确但需更多样本）
    
    'layers': ['layer2', 'layer3'],  # 特征层
                                      # layer2: 中层特征，捕捉纹理
                                      # layer3: 深层特征，捕捉形状
    
    'coreset_sampling_ratio': 0.1,   # 核心集采样比例：10%
                                      # 小样本时用较大比例保留更多信息
                                      # 大样本可降至 0.01-0.05
    
    'num_neighbors': 9,              # K近邻数量
                                      # 影响异常分数的平滑度
    
    # ===== 数据配置 =====
    'image_size': (512, 512),        # 输入尺寸
                                      # 平衡精度和速度
                                      # 更大尺寸（1024）可检测更小缺陷
    
    'train_batch_size': 1,           # 训练批次
                                      # PatchCore 内存占用大，设为 1
    
    'eval_batch_size': 1,            # 评估批次
    
    'num_workers': 8,                # 数据加载线程数
                                      # Windows 下如有问题可设为 0
    
    # ===== 数据增强（必须启用！） =====
    'augmentation': True,
    'augmentation_config': {
        # 几何变换
        'horizontal_flip': True,
        'vertical_flip': True,
        'rotate_90': True,
        'shift_scale_rotate': {
            'shift_limit': 0.05,
            'scale_limit': 0.1,
            'rotate_limit': 15,
        },
        # 颜色变换
        'brightness_contrast': {
            'brightness_limit': 0.2,
            'contrast_limit': 0.2,
        },
        'gaussian_noise': True,
        'gaussian_blur': True,
    },
    
    # ===== 后处理配置 =====
    'normalization': 'min_max',      # 异常分数归一化：Min-Max
    'threshold': 'adaptive',         # 阈值方法：自适应
    
    # ===== 输出配置 =====
    'save_heatmap': True,            # 保存热力图可视化
    'save_predictions': True,        # 保存预测结果
}


# ============================================================================
# 【如何应用配置到训练脚本】
# ============================================================================

"""
# 方式1：直接在 train.py 中使用配置
# ----------------------------------------------------------------------------

from anomalib.models.patchcore import Patchcore
from anomalib.data.folder import Folder
from anomalib.data.task_type import TaskType
import albumentations as A

# 使用配置创建数据增强
if recommended_config['augmentation']:
    aug_cfg = recommended_config['augmentation_config']
    train_transform = A.Compose([
        A.HorizontalFlip(p=0.5) if aug_cfg['horizontal_flip'] else A.NoOp(),
        A.VerticalFlip(p=0.5) if aug_cfg['vertical_flip'] else A.NoOp(),
        A.RandomRotate90(p=0.5) if aug_cfg['rotate_90'] else A.NoOp(),
        A.ShiftScaleRotate(**aug_cfg['shift_scale_rotate'], p=0.5),
        A.RandomBrightnessContrast(**aug_cfg['brightness_contrast'], p=0.5),
        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3) if aug_cfg['gaussian_noise'] else A.NoOp(),
        A.GaussianBlur(blur_limit=(3, 5), p=0.2) if aug_cfg['gaussian_blur'] else A.NoOp(),
    ])
else:
    train_transform = None

# 使用配置创建数据模块
datamodule = Folder(
    normal_dir="./datasets/task_1/good",
    abnormal_dir="./datasets/task_1/defect",
    mask_dir="./datasets/task_1/mask/defect",
    image_size=recommended_config['image_size'],
    train_batch_size=recommended_config['train_batch_size'],
    eval_batch_size=recommended_config['eval_batch_size'],
    num_workers=recommended_config['num_workers'],
    task=TaskType.SEGMENTATION,
    transform_config_train=train_transform,
)

# 使用配置创建模型
model = Patchcore(
    input_size=recommended_config['image_size'],
    backbone=recommended_config['backbone'],
    layers=recommended_config['layers'],
    pre_trained=True,
    coreset_sampling_ratio=recommended_config['coreset_sampling_ratio'],
    num_neighbors=recommended_config['num_neighbors'],
)

# ----------------------------------------------------------------------------

# 方式2：使用 YAML 配置文件
# ----------------------------------------------------------------------------
# 参见 demo/config/default_config.yaml
"""
```

---

## 8. Demo 使用指南

### 8.1 文件结构

```
haikang/
├── demo/
│   ├── advanced_train.py          # 高级训练脚本（含数据增强、域自适应）
│   ├── adaptive_inference.py      # 自适应推理脚本
│   ├── explainability.py          # 可解释性分析脚本
│   ├── multi_task_train.py        # 多任务批量训练
│   └── config/
│       └── default_config.yaml    # 默认配置文件
├── datasets/
│   └── task_1~10/
└── results/
```

### 8.2 快速开始

```bash
# 1. 安装依赖
pip install anomalib==0.7.0 albumentations

# 2. 单任务训练
python demo/advanced_train.py --task task_1

# 3. 多任务批量训练
python demo/multi_task_train.py --tasks all

# 4. 自适应推理
python demo/adaptive_inference.py --model results/task_1/weights/model.ckpt --image test.png

# 5. 可解释性分析
python demo/explainability.py --model results/task_1/weights/model.ckpt --image test.png
```

### 8.3 配置文件说明

参见 `demo/config/default_config.yaml`

---

## 附录 A: 常见问题与解决方案

### Q1: 训练样本过少导致效果差

**解决方案**:
1. 启用数据增强
2. 使用更轻量的主干网络（resnet18）
3. 增大 coreset_sampling_ratio
4. 考虑使用 PaDiM（对样本数量更不敏感）

### Q2: 产品批次变化导致误检

**解决方案**:
1. 启用增量学习，用新批次正常样本更新记忆库
2. 使用特征归一化减轻域偏移
3. 增加数据增强多样性

### Q3: 推理速度慢

**解决方案**:
1. 减少 coreset 大小
2. 使用更小的输入尺寸
3. 导出为 ONNX/TensorRT 格式
4. 考虑使用 EfficientAd 或 FastFlow

### Q4: 内存不足

**解决方案**:
1. 减少 coreset_sampling_ratio
2. 减少 batch_size
3. 使用混合精度训练

---

## 附录 B: 参考文献

1. Roth, K., et al. "Towards Total Recall in Industrial Anomaly Detection." CVPR 2022.
2. Defard, T., et al. "PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization." ICPR 2021.
3. Zavrtanik, V., et al. "DRAEM-A discriminatively trained reconstruction embedding for surface anomaly detection." ICCV 2021.
4. Yu, J., et al. "FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows." arXiv 2021.

---

*文档版本: 1.1*
*最后更新: 2026年2月1日*
*适用: anomalib 0.7.0*
*更新内容: 为所有代码添加详细注释和集成说明*
